#!/usr/bin/env python3
"""Build persistence sensitivity analysis across multiple tropical cyclone events.

Reads previously generated time_summary.csv files for varying --persistence-periods
values and produces:
  - reports/persistence_sensitivity_analysis.csv (summary table)
  - reports/persistence_sensitivity_plot.png (visualization of timing deltas)

Columns produced:
Event,PersistencePeriods,AlgoSignalStart,AlgoSignalEnd,AlgoDuration(min),DeltaStart_vs_Official(min),DeltaEnd_vs_Official(min),Comment

Assumptions:
- Directories already generated by running analyze_typhoon.py with proper arguments:
    reports/{event}_sensitivity/persistence_{N}_periods/time_summary.csv
- Official Signal 8 start/end times sourced from validation summaries.
- Datetimes are in local HKT and treated as naive timestamps here.

"""

from __future__ import annotations

from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

BASE = Path(".")
REPORTS = BASE / "reports"
REPORTS.mkdir(exist_ok=True)

# Official timings (from previously generated validation summaries)
OFFICIAL = {
    "Talim": {
        "start": pd.Timestamp("2023-07-17 00:40:00"),
        "end": pd.Timestamp("2023-07-17 16:20:00"),
    },
    "Tapah": {
        "start": pd.Timestamp("2025-09-07 21:20:00"),
        "end": pd.Timestamp("2025-09-08 13:10:00"),
    },
}

PERSISTENCE_VALUES = [1, 2, 3, 4]
EVENTS = ["Talim", "Tapah"]

records = []
for event in EVENTS:
    for p in PERSISTENCE_VALUES:
        time_csv = (
            REPORTS
            / f"{event.lower()}_sensitivity"
            / f"persistence_{p}_periods"
            / "time_summary.csv"
        )
        if not time_csv.exists():
            records.append(
                {
                    "Event": event,
                    "PersistencePeriods": p,
                    "AlgoSignalStart": "MISSING",
                    "AlgoSignalEnd": "MISSING",
                    "AlgoDuration(min)": 0,
                    "DeltaStart_vs_Official(min)": np.nan,
                    "DeltaEnd_vs_Official(min)": np.nan,
                    "Comment": "time_summary.csv not found",
                }
            )
            continue
        df = pd.read_csv(time_csv, parse_dates=["datetime"])
        # Persistent definition: use persistent_T8 True rows
        persistent_rows = df[df.get("persistent_T8", False)]
        if persistent_rows.empty:
            records.append(
                {
                    "Event": event,
                    "PersistencePeriods": p,
                    "AlgoSignalStart": "NONE",
                    "AlgoSignalEnd": "NONE",
                    "AlgoDuration(min)": 0,
                    "DeltaStart_vs_Official(min)": np.nan,
                    "DeltaEnd_vs_Official(min)": np.nan,
                    "Comment": "No persistent T8 detected (max consecutive qualifying periods: %d)"
                    % int(df.get("consecutive_periods_above_T8", pd.Series([0])).max()),
                }
            )
            continue
        algo_start = persistent_rows.iloc[0]["datetime"]
        algo_end = persistent_rows.iloc[-1]["datetime"]
        algo_duration_min = int((algo_end - algo_start).total_seconds() // 60)
        off = OFFICIAL[event]
        delta_start = int((algo_start - off["start"]).total_seconds() // 60)
        delta_end = int((algo_end - off["end"]).total_seconds() // 60)
        comment_parts = []
        if delta_start >= 0:
            comment_parts.append(f"Start delayed {delta_start} min vs official")
        else:
            comment_parts.append(f"Start early {abs(delta_start)} min vs official")
        if delta_end >= 0:
            comment_parts.append(f"End lag {delta_end} min vs official")
        else:
            comment_parts.append(f"Ended early {abs(delta_end)} min vs official")
        comment_parts.append(f"Duration {algo_duration_min} min")
        records.append(
            {
                "Event": event,
                "PersistencePeriods": p,
                "AlgoSignalStart": algo_start.strftime("%Y-%m-%d %H:%M:%S"),
                "AlgoSignalEnd": algo_end.strftime("%Y-%m-%d %H:%M:%S"),
                "AlgoDuration(min)": algo_duration_min,
                "DeltaStart_vs_Official(min)": delta_start,
                "DeltaEnd_vs_Official(min)": delta_end,
                "Comment": "; ".join(comment_parts),
            }
        )

out_df = pd.DataFrame(records)
out_csv = REPORTS / "persistence_sensitivity_analysis.csv"
out_df.to_csv(out_csv, index=False)
print(f"Wrote: {out_csv}")

# Plot timing deltas
fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)
for i, metric in enumerate(
    ["DeltaStart_vs_Official(min)", "DeltaEnd_vs_Official(min)"]
):
    ax = axes[i]
    for event in EVENTS:
        sub = out_df[out_df.Event == event]
        ax.plot(
            sub["PersistencePeriods"],
            sub[metric],
            marker="o",
            label=event,
        )
    ax.axhline(0, color="black", linewidth=0.8, linestyle="--", alpha=0.6)
    ax.set_title(
        metric.replace("_vs_Official(min)", " Delta (min)").replace("Delta", "Timing")
    )
    ax.set_xlabel("Persistence periods (10-min snapshots)")
    ax.set_ylabel("Delta (minutes) vs official")
    ax.grid(True, linestyle=":", alpha=0.5)
    ax.legend()

fig.suptitle("Effect of Persistence Threshold on Algorithm Signal Timing (T8)")
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plot_path = REPORTS / "persistence_sensitivity_plot.png"
fig.savefig(plot_path, dpi=150)
print(f"Wrote: {plot_path}")
